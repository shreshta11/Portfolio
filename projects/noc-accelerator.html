<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>NoC DNN Accelerator using Processing-in-Memory (PIM) — Shreshta Prabhu</title>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;800&display=swap" rel="stylesheet">
  <style>
    :root{--bg:#0b0d12;--card:#111318;--border:#1f232b;--muted:#aab3bf;--text:#e9edf1;--brand:#7bb3ff;--chip:#151923}
    *{box-sizing:border-box}
    body{margin:0;background:var(--bg);color:var(--text);font-family:Inter,system-ui;line-height:1.65}
    a{color:#7bb3ff;text-decoration:none} a:hover{text-decoration:underline}
    .wrap{max-width:980px;margin:0 auto;padding:28px 20px 64px}
    header{display:flex;gap:12px;align-items:center;margin-bottom:18px}
    .crumb{font-size:14px;color:#aab3bf}
    h1{font-size:40px;line-height:1.1;margin:8px 0 14px}
    h2{font-size:22px;margin:0 0 8px}
    h3{font-size:18px;margin:0 0 8px}
    .meta{color:#aab3bf;font-size:15px;margin-bottom:16px}
    .tldr{background:#111318;border:1px solid #1f232b;border-radius:14px;padding:12px 14px;margin:14px 0}
    .chips{display:flex;flex-wrap:wrap;gap:8px;margin:8px 0 14px}
    .chip{font-size:13px;background:#151923;border:1px solid #1f232b;padding:6px 10px;border-radius:999px}
    .card{background:#111318;border:1px solid #1f232b;border-radius:16px;padding:16px;margin:16px 0}
    figure{margin:0}
    img{max-width:100%;height:auto;border-radius:12px;border:1px solid #1f232b;background:#0e1118}
    figcaption{color:#aab3bf;font-size:13px;margin-top:6px}
    .metrics{display:grid;grid-template-columns:repeat(3,minmax(0,1fr));gap:12px}
    .metric{background:#0e1118;border:1px solid #1f232b;border-radius:12px;padding:10px;font-size:14px}
    .metric b{display:block;font-size:18px;margin-bottom:2px}
    details.more{margin-top:8px}
    details.more > summary{cursor:pointer;list-style:none;padding:10px 12px;border:1px solid #1f232b;border-radius:10px;background:#0e1118;color:#e9edf1}
    details.more[open] > summary{border-bottom-left-radius:0;border-bottom-right-radius:0}
    details.more .more-body{border:1px solid #1f232b;border-top:0;border-bottom-left-radius:10px;border-bottom-right-radius:10px;padding:12px;background:#0f1219}
    ul{margin:8px 0 0 18px} li{margin:6px 0}
    footer{margin-top:48px;color:#aab3bf;font-size:14px}
    @media (max-width:760px){.metrics{grid-template-columns:1fr 1fr}}
    @media (max-width:520px){.metrics{grid-template-columns:1fr}}
  </style>
</head>
<body>
  <div class="wrap">
    <header>
      <a class="crumb" href="../projects_page_shreshta_projects.html" onclick="if(history.length>1){history.back();return false;}">← Back to projects</a>
    </header>

    <h1>NoC DNN Accelerator using Processing-in-Memory</h1>
    <div class="meta">SEEDER Group, National University of Singapore — May 2025–July 2025</div>

    <div class="tldr">
      <strong>Goal:</strong> prototype a PIM-NoC accelerator for DNN inference that targets lower energy, lower latency, and higher
      throughput than conventional CPU/GPU baselines. The design explores memory choices, NoC topologies, layer
      mapping, routing, quantization, and an RTL micro-prototype that computes single-head attention.
    </div>

    <div class="chips">
      <span class="chip">Role: Architecture &amp; RTL simulation</span>
      <span class="chip">Focus: PIM + NoC for DNNs</span>
      <span class="chip">Dataflow: Weight-stationary</span>
      <span class="chip">Routing: X–Y (deterministic)</span>
      <span class="chip">Quantization: Q16.16 fixed-point</span>
      <span class="chip">Tools: Verilog/RTL, Python, PyTorch (baseline)</span>
    </div>

    <div class="card">
      <h3>High-Level Architecture</h3>
      <p>
        Evaluated PIM options (DRAM/SRAM/HBM vs. emerging ReRAM/PCM) and NoC organizations (2D mesh vs. 3D stacked
        logic+DRAM). The prototype uses a small PIM-NoC built from DRAM arrays, each node providing in-memory add/mul
        and attached to a lightweight router. Nodes 1 &amp; 6 interface external memory for weights/inputs; the network uses
        deterministic X–Y routing. <!-- add a diagram here if you have one -->
      </p>
      <figure>
        <img src="noc_block.png" alt="Block diagram of the PIM-NoC: DRAM PIM nodes connected by a mesh with external memory at two edge nodes" loading="lazy" decoding="async">
        <figcaption>Concept diagram — PIM nodes (DRAM + simple ALU) connected by a 2D mesh; edge nodes bridge to external memory.</figcaption>
      </figure>
    </div>

    <div class="card">
      <h3>Dataflow &amp; Mapping</h3>
      <ul>
        <li><strong>Layer mapping:</strong> weight-stationary (maximize weight reuse; reduce inter-node traffic).</li>
        <li><strong>Tiling:</strong> weights &amp; inputs split into four blocks across nodes; broadcasts used for inputs.</li>
        <li><strong>Routing:</strong> X–Y deterministic for simplicity &amp; deadlock-free operation.</li>
        <li><strong>Quantization:</strong> 32-bit fixed-point (Q16.16) in the micro-prototype; roadmap includes 8-bit FP.</li>
      </ul>
    </div>

    <div class="card">
      <h3>RTL Micro-Prototype</h3>
      <p>
        Implemented a 6-node PIM-NoC that performs single-head attention. Each node has a 32×32-bit DRAM scratchpad with
        adjacent-row add/multiply, a simple router, and control FSMs. External logic orchestrates broadcasts, matrix ops, and SoftMax.
      </p>
      <div class="metrics">
        <div class="metric"><b>6</b> PIM nodes (2 with external I/O)</div>
        <div class="metric"><b>32×32-bit</b> DRAM per node</div>
        <div class="metric"><b>3170</b> total cycles (attention)</div>
      </div>
      <figure>
        <img src="rtl_wave.png" alt="Waveform/trace from RTL simulation showing broadcast, matrix multiply/add, softmax, and egress phases" loading="lazy" decoding="async">
        <figcaption>RTL trace — broadcast, matrix multiply/add, SoftMax, and egress phases across the mesh.</figcaption>
      </figure>
    </div>

    <div class="card">
      <h3>Results</h3>
      <ul>
        <li><strong>Latency (RTL):</strong> 3,170 cycles for a single-head attention workload; cycle breakdown — broadcast (31%), matrix multiply (26%), matrix add (20%), data moves (23%).</li>
        <li><strong>Energy vs. CPU baseline:</strong> ~35&nbsp;µJ (PIM-NoC) vs. ~75&nbsp;µJ (PC CPU model) for the same attention kernel.</li>
        <li><strong>Network activity:</strong> 296 router hops and 224 intra-DRAM data moves for a single pass.</li>
      </ul>
    </div>

    <details class="more">
      <summary><strong>Click for observations &amp; improvements</strong></summary>
      <div class="more-body">
        <div class="card">
          <h3>Hotspots &amp; Fixes</h3>
          <ul>
            <li><strong>Ingress bottleneck:</strong> external inputs funneled through a single edge node. <em>Mitigation:</em> add more external ports; adopt adaptive routing.</li>
            <li><strong>Intra-DRAM reshuffles:</strong> 23% cycles spent moving partials due to tile limits. <em>Mitigation:</em> 8-bit FP quantization; larger per-node memory to reduce reshuffles.</li>
            <li><strong>Duplicated MM/A around SoftMax:</strong> increase node count to parallelize; reduces MM/A time at the cost of more traffic.</li>
            <li><strong>SoftMax staging:</strong> integrate FlashAttention-style streaming to avoid buffer reshuffles.</li>
          </ul>
        </div>

        <div class="card">
          <h3>What’s Next</h3>
          <ul>
            <li>Evaluate 3D stacked DRAM + logic and router-with-compute designs.</li>
            <li>Move from Q16.16 to 8-bit FP with minimal accuracy loss; re-run energy/latency.</li>
            <li>Replace X–Y with adaptive routing; formal deadlock analysis + traffic benchmarks.</li>
          </ul>
        </div>
      </div>
    </details>

    <div class="card">
      <h3>Artifacts</h3>
      <ul>
        <li><a href="NOC.pdf">Report (PDF)</a></li>
        <!-- Add RTL source or notebooks links here if public -->
      </ul>
    </div>

    <footer>© 2025 Shreshta Prabhu</footer>
  </div>
</body>
</html>

