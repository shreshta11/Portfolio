<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>NoC DNN Accelerator using Processing-in-Memory(PIM) — Shreshta Prabhu</title>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;800&display=swap" rel="stylesheet">
  <style>
    :root{--bg:#0b0d12;--card:#111318;--stroke:#1f232b;--muted:#aab3bf;--fg:#e9edf1;--link:#7bb3ff}
    *{box-sizing:border-box}
    body{margin:0;background:var(--bg);color:var(--fg);font-family:Inter,system-ui;line-height:1.65}
    a{color:var(--link);text-decoration:none} a:hover{text-decoration:underline}
    .wrap{max-width:900px;margin:0 auto;padding:28px 20px 64px}
    header{display:flex;gap:12px;align-items:center;margin-bottom:18px}
    .crumb{font-size:14px;color:var(--muted)}
    h1{font-size:40px;line-height:1.1;margin:8px 0 14px}
    h2,h3{margin:0 0 8px}
    .meta{color:var(--muted);font-size:15px;margin-bottom:18px}
    .card{background:var(--card);border:1px solid var(--stroke);border-radius:16px;padding:18px;margin:20px 0}
    .hero{overflow:hidden;border-radius:16px;border:1px solid var(--stroke);margin:14px 0 20px}
    img{max-width:100%;display:block}
    figure{margin:0}
    figcaption{color:var(--muted);font-size:13px;margin-top:6px}
    footer{margin-top:48px;color:var(--muted);font-size:14px}
    /* simple media grid */
    .grid{display:grid;gap:12px}
    @media (min-width:720px){.grid-2{grid-template-columns:1fr 1fr}}
    .pill{display:inline-block;padding:4px 10px;border:1px solid var(--stroke);border-radius:999px;color:var(--muted);font-size:12px;margin-right:8px}
    ul{margin:0 0 8px 18px}
    p{margin:0 0 10px}
  </style>
</head>
<body>
  <div class="wrap">
    <header>
      <a class="crumb" href="../index.html">← Back to projects</a>
    </header>

    <h1>NoC Accelerator using Processing-in-Memory</h1>
    <div class="meta">SEEDER Group, National University of Singapore - May 2025 to July 2025</div>

    <!-- Intro + Hero -->
    <p>
      The rapid scaling of large language models is exposing fundamental energy and memory-bandwidth limits in GPU pipelines. To address this, at the SEEDER Lab,we  research on a hardware-software co-design of a DNN Accelerator to improve inference efficiency as well as energy. 

 In my internship, I designed and simulated a <strong>Processing-in-Memory (PIM) Network-on-Chip (NoC) accelerator</strong> 
  that integrates compute into DRAM arrays and optimizes on-chip dataflow. 
<p>
This approach reduces costly data movement and 
  lowers per-inference latency and energy. Below, I outline the architecture, dataflow strategies, RTL prototype, and the 
  bottlenecks encountered along with the fixes that proved effective.
    </p>

    <div class="hero">
      <figure>
        <img src="hero_pim_noc.png" alt="High-level PIM–NoC concept showing memory-centric compute and mesh interconnect">
        <figcaption>6-node PIM–NoC Block Diagram: compute near memory, routed over a lightweight on-chip mesh.</figcaption> 
      </p> 
     
      </figure>
    </div>


 <div class="hero">
      <figure>
      <img src="rtlsimulationresults.jpg" alt="High-level PIM–NoC concept showing memory-centric compute and mesh interconnect">
        <figcaption>Simulation Findings with benchmark comparison</figcaption>
     
      </figure>
    </div>

    <p>
      <span class="pill">PIM / DRAM</span>
      <span class="pill">NoC (X–Y vs Adaptive)</span>
      <span class="pill">LLM Inference</span>
      <span class="pill">Quantization</span>
      <span class="pill">RTL Simulation</span>
    </p>


  
    <div class="card">
      <h3>Design & Simulation</h3>
      <ul>
        <li>Explored memory technologies (DRAM/SRAM/HBM vs. ReRAM/PCM) and NoC topologies.</li>
        <li>Built a <strong>6-node PIM–NoC RTL prototype</strong> (32×32 DRAM arrays, weight-stationary dataflow, fixed-point arithmetic).</li>
        <li>Measured cycle-accurate latency and energy vs. CPU baseline; profiled router hops and internal data movement.</li>
      </ul>
    </div>

    <div class="card">
      <h3>High-Level System Design</h3>
      <p><strong>Memory Choices:</strong> Compared DRAM, SRAM, HBM, and emerging ReRAM/PCM technologies.</p>
      <p><strong>NoC Architecture:</strong> Explored 3D-stacked DRAM + logic meshes vs. router-based interconnects.</p>
      <p><strong>Goal:</strong> Balance cost, latency, fabrication yield, and scalability.</p>
    </div>

    <div class="card">
      <h3>Dataflow & Routing Strategies</h3>
      <p><strong>Layer Mapping:</strong> Implemented weight-stationary, output-stationary, and row-stationary approaches.</p>
      <p><strong>Routing:</strong> Evaluated deterministic X–Y vs. dynamic adaptive routing (pros/cons on congestion & deadlock risk).</p>
      <p><strong>Quantization:</strong> Demonstrated 8-bit fixed-point inference with negligible accuracy loss for tested CNN/LLM kernels.</p>
    </div>



  <details class="more">
  <summary><strong>Click for more details (requirements, design, validation) : </strong></summary>
  <div class="more-body">
   <div class="card">

      <div class="card">
      <h3>Dataflow & Routing Strategies</h3>
      <p><strong>Layer Mapping:</strong> Implemented weight-stationary, output-stationary, and row-stationary approaches.</p>
      <p><strong>Routing:</strong> Evaluated deterministic X–Y vs. dynamic adaptive routing (pros/cons on congestion & deadlock risk).</p>
      <p><strong>Quantization:</strong> Demonstrated 8-bit fixed-point inference with negligible accuracy loss for tested CNN/LLM kernels.</p>
    </div>


    <h3>Low-Level RTL Simulation</h3>
      <p><strong>Prototype:</strong> 6-node PIM–NoC model with 32×32-bit DRAM arrays.</p>
      <p><strong>Dataflow:</strong> Weight-stationary, X–Y routing, fixed-point arithmetic.</p>
      <p><strong>Results:</strong> 3,170 cycles per self-attention vs. 25,000 cycles on CPU; 35&nbsp;µJ vs. 75&nbsp;µJ energy.</p>
    </div>

    <div class="grid grid-2" style="margin-top:12px">
        <figure>
          <img src="results.png" alt="3D-stacked DRAM with logic layer cross-section">
          <figcaption>RTL Simulation Results </figcaption>
        </figure>
      </div>
     

    <div class="card">
      <h3>Benchmarking & Insights</h3>
      <p><strong>Cycle Breakdown:</strong> 31% broadcast input, 26% matrix multiplication, 23% data movement, 20% matrix addition.</p>
      <p><strong>Key Bottlenecks & Fixes:</strong></p>
      <ul>
        <li>Node-1 ingress bottleneck → add extra external links + adopt adaptive routing.</li>
        <li>Excessive intra-array shuffles → quantize + increase per-node memory to reduce remapping.</li>
        <li>SoftMax reshuffling → integrate FlashAttention-style streaming to overlap I/O and compute.</li>
      </ul>
    </div>

 <div class="hero">
      <figure>
      <img src="results.jpg" alt="High-level PIM–NoC concept showing memory-centric compute and mesh interconnect">
        <figcaption>Simulation Findings with benchmark comparison</figcaption>
     
      </figure>
    </div>


    

    <div class="card">
      <h3>My Role & Responsibilities</h3>
      <ul>
        <li>Led system design (memory selection, NoC architecture, dataflow strategy).</li>
        <li>Developed RTL simulations and instrumentation for cycle/energy profiling.</li>
        <li>Benchmarked against CPU/GPU baselines; analyzed router hops & DRAM movements.</li>
        <li>Identified bottlenecks and proposed fixes (links, adaptive routing, quantization, FlashAttention).</li>
        <li>Authored technical report and diagrams consolidating findings.</li>
      </ul>
    </div>


    
<footer style="margin-top:28px;color:#aab3bf;font-size:14px">© 2025 Shreshta Prabhu</footer>
  </div>
  
</body>
</html>
